{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MojTabaa4/NLP-Byte-Pair-Encoding-and-WordPiece-Tokenizer/blob/main/Byte-Pair-Encoding-and-WordPiece-Tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "LiQxgK1Tojas",
        "outputId": "775820d9-ca7b-4e09-ea48-38965d20e47b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.13.3\n"
          ]
        }
      ],
      "source": [
        "!pip install tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import json\n",
        "import re\n",
        "from collections import Counter\n",
        "from operator import itemgetter\n",
        "from pprint import pprint\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import requests\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE, WordPiece\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import BpeTrainer, WordPieceTrainer\n"
      ],
      "metadata": {
        "id": "fds5q7EyovJ7"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_corpus(file_name: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Reads a text file and returns a list of words.\n",
        "    \"\"\"\n",
        "    with open(file_name, \"r\", encoding=\"utf-8\") as file:\n",
        "        words_list = file.read().split()\n",
        "\n",
        "    return words_list"
      ],
      "metadata": {
        "id": "I59j7F4How_U"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_name: str = \"text.txt\"\n",
        "all_words: List[str] = read_corpus(file_name)\n",
        "print(all_words)"
      ],
      "metadata": {
        "id": "mDNTwMYPv_xk",
        "outputId": "0e89612b-969f-4730-ab77-2514aa447968",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['low', 'lower', 'newest', 'low', 'lower', 'newest', 'low', 'widest', 'newest', 'low', 'widest', 'newest', 'low', 'widest', 'newest']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def word_frequency(words: List[str]) -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    Calculate frequency of words in a list.\n",
        "    \"\"\"\n",
        "    count = dict(Counter(words))\n",
        "    return count"
      ],
      "metadata": {
        "id": "Z1dCVTVaow1m"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_freqs = word_frequency(all_words)\n",
        "\n",
        "print(word_freqs)"
      ],
      "metadata": {
        "id": "PtWZmWmFv7iA",
        "outputId": "a149d29c-0955-456b-deab-cc87b3c6e808",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'low': 5, 'lower': 2, 'newest': 5, 'widest': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_end_of_word_symbol(freqs: Dict[str, int]) -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    Adds the end-of-word symbol \"</w>\" to each word in the frequency dictionary\n",
        "    and returns a new dictionary with the modified keys.\n",
        "    \"\"\"\n",
        "    word_freq = {}\n",
        "    for word, freq in freqs.items():\n",
        "        word_freq[\" \".join(list(word)) + \" </w>\"] = freq\n",
        "    \n",
        "    return word_freq\n"
      ],
      "metadata": {
        "id": "ia_2gpTuowzO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_freq = add_end_of_word_symbol(word_freqs)\n",
        "\n",
        "pprint(word_freq)"
      ],
      "metadata": {
        "id": "JhlouOtfv2hK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stats(word_frequencies: Dict[str, int]) -> Dict[Tuple[str, str], int]:\n",
        "    \"\"\"Returns a dictionary of symbol pairs and their frequencies in the given words.\n",
        "\n",
        "    Args:\n",
        "        word_frequencies: A dictionary where keys are words and values are their frequencies.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary where keys are symbol pairs and values are their frequencies.\n",
        "    \"\"\"\n",
        "    symbol_pairs_frequency = collections.defaultdict(int)\n",
        "\n",
        "    for word, frequency in word_frequencies.items():\n",
        "        symbols = word.split()\n",
        "\n",
        "        for i in range(len(symbols) - 1):\n",
        "            symbol_pair = (symbols[i], symbols[i + 1])\n",
        "            symbol_pairs_frequency[symbol_pair] += frequency\n",
        "\n",
        "    return symbol_pairs_frequency"
      ],
      "metadata": {
        "id": "DAci6dZSoww-"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(get_stats(word_freq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TW84C13ZY90D",
        "outputId": "45c8f8f4-8d07-4883-9ebf-fbec601ae83f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<class 'int'>,\n",
            "            {('d', 'e'): 3,\n",
            "             ('e', 'r'): 2,\n",
            "             ('e', 's'): 8,\n",
            "             ('e', 'w'): 5,\n",
            "             ('i', 'd'): 3,\n",
            "             ('l', 'o'): 7,\n",
            "             ('n', 'e'): 5,\n",
            "             ('o', 'w'): 7,\n",
            "             ('r', '</w>'): 2,\n",
            "             ('s', 't'): 8,\n",
            "             ('t', '</w>'): 8,\n",
            "             ('w', '</w>'): 5,\n",
            "             ('w', 'e'): 7,\n",
            "             ('w', 'i'): 3})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_vocab(pair: Tuple[str, str], vocab: Dict[str, int]) -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    Merge a pair of symbols into a single symbol in a vocabulary dictionary.\n",
        "\n",
        "    Args:\n",
        "        pair: A tuple of two symbols to be merged.\n",
        "        vocab: A dictionary where the keys are words and the values are their frequencies.\n",
        "\n",
        "    Returns:\n",
        "        A new dictionary where each instance of the pair in the keys of the input dictionary has been replaced with the\n",
        "        concatenated string of the pair.\n",
        "    \"\"\"\n",
        "    pair_str = ' '.join(pair)\n",
        "    new_vocab = {\n",
        "        re.sub(re.escape(pair_str), ''.join(pair), word): freq for word, freq in vocab.items()\n",
        "        }    \n",
        "    \n",
        "    return new_vocab"
      ],
      "metadata": {
        "id": "9hlldjiQtFJW"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_MERGES = 10\n",
        "\n",
        "def train(word_freq: Dict[str, int]) -> Dict[Tuple[str, str], int]:\n",
        "    \"\"\"\n",
        "    Train a byte pair encoding model on a word frequency dictionary.\n",
        "\n",
        "    Args:\n",
        "        word_freq: A dictionary where the keys are words and the values are their frequencies.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary where the keys are symbol pairs (tuples) and the values are the iteration numbers at which they were merged.\n",
        "    \"\"\"\n",
        "    all_merge_steps: Dict[Tuple[str, str], int] = {}\n",
        "\n",
        "    for i, _ in enumerate(range(MAX_MERGES)):\n",
        "        pair_stats = get_stats(word_freq)\n",
        "        \n",
        "        if not pair_stats:\n",
        "            print(\"Reached maximum iterations\")\n",
        "            break\n",
        "\n",
        "        best_pair = max(pair_stats, key=lambda x: pair_stats[x])\n",
        "        all_merge_steps[best_pair] = i\n",
        "        print(f\"Most frequent pair at iteration {i + 1}: {best_pair}\")\n",
        "        word_freq = merge_vocab(best_pair, word_freq)\n",
        "        print(f\"Vocabulary at iteration {i + 1}:\")\n",
        "        print(json.dumps(word_freq, indent=4) + '\\n')\n",
        "\n",
        "    return all_merge_steps"
      ],
      "metadata": {
        "id": "YZgnAp17tFHA"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_merge_steps = train(word_freq)"
      ],
      "metadata": {
        "id": "Emc-S-1WvrE3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a59ce41b-0590-4bbf-fa4a-794fdcfeb85a"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most frequent pair at iteration 1: ('e', 's')\n",
            "Vocabulary at iteration 1:\n",
            "{\n",
            "    \"l o w </w>\": 5,\n",
            "    \"l o w e r </w>\": 2,\n",
            "    \"n e w es t </w>\": 5,\n",
            "    \"w i d es t </w>\": 3\n",
            "}\n",
            "\n",
            "Most frequent pair at iteration 2: ('es', 't')\n",
            "Vocabulary at iteration 2:\n",
            "{\n",
            "    \"l o w </w>\": 5,\n",
            "    \"l o w e r </w>\": 2,\n",
            "    \"n e w est </w>\": 5,\n",
            "    \"w i d est </w>\": 3\n",
            "}\n",
            "\n",
            "Most frequent pair at iteration 3: ('est', '</w>')\n",
            "Vocabulary at iteration 3:\n",
            "{\n",
            "    \"l o w </w>\": 5,\n",
            "    \"l o w e r </w>\": 2,\n",
            "    \"n e w est</w>\": 5,\n",
            "    \"w i d est</w>\": 3\n",
            "}\n",
            "\n",
            "Most frequent pair at iteration 4: ('l', 'o')\n",
            "Vocabulary at iteration 4:\n",
            "{\n",
            "    \"lo w </w>\": 5,\n",
            "    \"lo w e r </w>\": 2,\n",
            "    \"n e w est</w>\": 5,\n",
            "    \"w i d est</w>\": 3\n",
            "}\n",
            "\n",
            "Most frequent pair at iteration 5: ('lo', 'w')\n",
            "Vocabulary at iteration 5:\n",
            "{\n",
            "    \"low </w>\": 5,\n",
            "    \"low e r </w>\": 2,\n",
            "    \"n e w est</w>\": 5,\n",
            "    \"w i d est</w>\": 3\n",
            "}\n",
            "\n",
            "Most frequent pair at iteration 6: ('low', '</w>')\n",
            "Vocabulary at iteration 6:\n",
            "{\n",
            "    \"low</w>\": 5,\n",
            "    \"low e r </w>\": 2,\n",
            "    \"n e w est</w>\": 5,\n",
            "    \"w i d est</w>\": 3\n",
            "}\n",
            "\n",
            "Most frequent pair at iteration 7: ('n', 'e')\n",
            "Vocabulary at iteration 7:\n",
            "{\n",
            "    \"low</w>\": 5,\n",
            "    \"low e r </w>\": 2,\n",
            "    \"ne w est</w>\": 5,\n",
            "    \"w i d est</w>\": 3\n",
            "}\n",
            "\n",
            "Most frequent pair at iteration 8: ('ne', 'w')\n",
            "Vocabulary at iteration 8:\n",
            "{\n",
            "    \"low</w>\": 5,\n",
            "    \"low e r </w>\": 2,\n",
            "    \"new est</w>\": 5,\n",
            "    \"w i d est</w>\": 3\n",
            "}\n",
            "\n",
            "Most frequent pair at iteration 9: ('new', 'est</w>')\n",
            "Vocabulary at iteration 9:\n",
            "{\n",
            "    \"low</w>\": 5,\n",
            "    \"low e r </w>\": 2,\n",
            "    \"newest</w>\": 5,\n",
            "    \"w i d est</w>\": 3\n",
            "}\n",
            "\n",
            "Most frequent pair at iteration 10: ('w', 'i')\n",
            "Vocabulary at iteration 10:\n",
            "{\n",
            "    \"low</w>\": 5,\n",
            "    \"low e r </w>\": 2,\n",
            "    \"newest</w>\": 5,\n",
            "    \"wi d est</w>\": 3\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(original_word: str, all_merge_steps: Dict[Tuple[str, str], int]) -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    Encode a word using a byte pair encoding model.\n",
        "\n",
        "    Args:\n",
        "        original_word: The word to be encoded.\n",
        "        all_merge_steps: A dictionary where the keys are symbol pairs (tuples) and the values are the iteration numbers at which they were merged.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary where the keys are the symbols in the encoded word and the values are their frequencies.\n",
        "    \"\"\"\n",
        "    if len(original_word) == 1:\n",
        "        return {original_word: 1}\n",
        "\n",
        "    vocab: Dict[str, int] = {original_word: 1}\n",
        "    vocab = add_end_of_word_symbol(vocab)\n",
        "    print(f'{vocab=}')\n",
        "\n",
        "    candidate_pairs = []\n",
        "    print(f'{all_merge_steps=}\\n')\n",
        "\n",
        "    while True:\n",
        "        symbol_pairs = get_stats(vocab)\n",
        "        print(f'{symbol_pairs=}')\n",
        "        candidate_pairs = [(pair, all_merge_steps[pair]) for pair in symbol_pairs if pair in all_merge_steps]\n",
        "        print(f'{candidate_pairs=}')\n",
        "\n",
        "        \n",
        "        if not candidate_pairs:\n",
        "            break\n",
        "        \n",
        "        best_pair = min(candidate_pairs, key=itemgetter(1))[0]\n",
        "        print(f\"pair to merge: {best_pair}\\n\")\n",
        "        vocab = merge_vocab(best_pair, vocab)\n",
        "\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "dhbqqtX8tFE0"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_word = 'lowest'\n",
        "encode(original_word, all_merge_steps)"
      ],
      "metadata": {
        "id": "KXgXsnrTtFC0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29eaa6d8-2da9-451d-84d0-58a70c16c649"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab={'l o w e s t </w>': 1}\n",
            "all_merge_steps={('e', 's'): 0, ('es', 't'): 1, ('est', '</w>'): 2, ('l', 'o'): 3, ('lo', 'w'): 4, ('low', '</w>'): 5, ('n', 'e'): 6, ('ne', 'w'): 7, ('new', 'est</w>'): 8, ('w', 'i'): 9}\n",
            "\n",
            "symbol_pairs=defaultdict(<class 'int'>, {('l', 'o'): 1, ('o', 'w'): 1, ('w', 'e'): 1, ('e', 's'): 1, ('s', 't'): 1, ('t', '</w>'): 1})\n",
            "candidate_pairs=[(('l', 'o'), 3), (('e', 's'), 0)]\n",
            "pair to merge: ('e', 's')\n",
            "\n",
            "symbol_pairs=defaultdict(<class 'int'>, {('l', 'o'): 1, ('o', 'w'): 1, ('w', 'es'): 1, ('es', 't'): 1, ('t', '</w>'): 1})\n",
            "candidate_pairs=[(('l', 'o'), 3), (('es', 't'), 1)]\n",
            "pair to merge: ('es', 't')\n",
            "\n",
            "symbol_pairs=defaultdict(<class 'int'>, {('l', 'o'): 1, ('o', 'w'): 1, ('w', 'est'): 1, ('est', '</w>'): 1})\n",
            "candidate_pairs=[(('l', 'o'), 3), (('est', '</w>'), 2)]\n",
            "pair to merge: ('est', '</w>')\n",
            "\n",
            "symbol_pairs=defaultdict(<class 'int'>, {('l', 'o'): 1, ('o', 'w'): 1, ('w', 'est</w>'): 1})\n",
            "candidate_pairs=[(('l', 'o'), 3)]\n",
            "pair to merge: ('l', 'o')\n",
            "\n",
            "symbol_pairs=defaultdict(<class 'int'>, {('lo', 'w'): 1, ('w', 'est</w>'): 1})\n",
            "candidate_pairs=[(('lo', 'w'), 4)]\n",
            "pair to merge: ('lo', 'w')\n",
            "\n",
            "symbol_pairs=defaultdict(<class 'int'>, {('low', 'est</w>'): 1})\n",
            "candidate_pairs=[]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'low est</w>': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation of a BPE (Byte Pair Encoding) tokenizer using the HuggingFace Tokenizers library in Python."
      ],
      "metadata": {
        "id": "Pd9gdZmmlTh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize BPE tokenizer and trainer\n",
        "bpe_text_box_tokenizer = Tokenizer(BPE())\n",
        "bpe_text_box_tokenizer.pre_tokenizer = Whitespace()\n",
        "bpe_text_box_trainer = BpeTrainer()\n",
        "\n",
        "# Train tokenizer on sample text files\n",
        "files = [\"./Sample.txt\"]\n",
        "bpe_text_box_tokenizer.train(files, bpe_text_box_trainer)\n",
        "\n",
        "# Save tokenizer to JSON file\n",
        "bpe_text_box_tokenizer.save(\"./BPE-text-box.json\")\n",
        "\n",
        "# Print vocabulary size and tokens\n",
        "vocab_size = bpe_text_box_tokenizer.get_vocab_size()\n",
        "tokens = bpe_text_box_tokenizer.get_vocab()\n",
        "tokens = sorted(tokens.items(), key=lambda x: x[1])\n",
        "\n",
        "print(\"BPE - TextBox Tokenizer Results\")\n",
        "print(\"--------------------------------\")\n",
        "print(f\"Vocabulary Size: {vocab_size}\")\n",
        "print(\"Tokens:\")\n",
        "pprint(tokens[:20])\n"
      ],
      "metadata": {
        "id": "hvrF932LlK9Y",
        "outputId": "f2dcac8b-943e-4bb8-b239-5ade3b2dac68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BPE - TextBox Tokenizer Results\n",
            "--------------------------------\n",
            "Vocabulary Size: 113\n",
            "Tokens:\n",
            "[('!', 0),\n",
            " ('.', 1),\n",
            " ('?', 2),\n",
            " ('E', 3),\n",
            " ('L', 4),\n",
            " ('N', 5),\n",
            " ('P', 6),\n",
            " ('T', 7),\n",
            " ('W', 8),\n",
            " ('a', 9),\n",
            " ('b', 10),\n",
            " ('c', 11),\n",
            " ('d', 12),\n",
            " ('e', 13),\n",
            " ('f', 14),\n",
            " ('g', 15),\n",
            " ('h', 16),\n",
            " ('i', 17),\n",
            " ('k', 18),\n",
            " ('l', 19)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the tokenizers library to train a WordPiece tokenizer"
      ],
      "metadata": {
        "id": "A7Dx0uIPosF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and configure the WordPiece tokenizer\n",
        "wp_text_box_tokenizer = Tokenizer(WordPiece())\n",
        "wp_text_box_tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "# Train the WordPiece tokenizer on the sample text file\n",
        "wp_text_box_trainer = WordPieceTrainer()\n",
        "files = [f\"./Sample.txt\"]\n",
        "wp_text_box_tokenizer.train(files, wp_text_box_trainer)\n",
        "\n",
        "# Save the trained tokenizer to a JSON file\n",
        "wp_text_box_tokenizer.save(\"./WordPiece-text-box.json\")\n",
        "\n",
        "# Print vocabulary size and tokens\n",
        "vocab_size = wp_text_box_tokenizer.get_vocab_size()\n",
        "tokens = wp_text_box_tokenizer.get_vocab()\n",
        "tokens = sorted(tokens.items(), key=lambda x: x[1])\n",
        "\n",
        "# Print the number of extracted tokens and the extracted tokens themselves\n",
        "print(\"WordPiece - TextBox Tokenizer Results\")\n",
        "print(\"--------------------------------\")\n",
        "print(f\"Vocabulary Size: {vocab_size}\")\n",
        "print(\"Tokens:\")\n",
        "pprint(tokens[:20])"
      ],
      "metadata": {
        "id": "5LMxB_dznmpM",
        "outputId": "b64de91a-c471-49e9-a9e5-81c95e08403f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WordPiece - TextBox Tokenizer Results\n",
            "--------------------------------\n",
            "Vocabulary Size: 139\n",
            "Tokens:\n",
            "[('!', 0),\n",
            " ('.', 1),\n",
            " ('?', 2),\n",
            " ('E', 3),\n",
            " ('L', 4),\n",
            " ('N', 5),\n",
            " ('P', 6),\n",
            " ('T', 7),\n",
            " ('W', 8),\n",
            " ('a', 9),\n",
            " ('b', 10),\n",
            " ('c', 11),\n",
            " ('d', 12),\n",
            " ('e', 13),\n",
            " ('f', 14),\n",
            " ('g', 15),\n",
            " ('h', 16),\n",
            " ('i', 17),\n",
            " ('k', 18),\n",
            " ('l', 19)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jW3QEkx7o0Re"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}