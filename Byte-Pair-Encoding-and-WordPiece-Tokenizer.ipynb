{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MojTabaa4/NLP-Byte-Pair-Encoding-and-WordPiece-Tokenizer/blob/main/Byte-Pair-Encoding-and-WordPiece-Tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Byte Pair Encoding (BPE) for Text Compression\n",
        "\n",
        "Byte Pair Encoding (BPE) is a data compression technique that works by replacing repeated sequences of characters with a single token. In this notebook, we will implement a BPE algorithm from scratch using Python."
      ],
      "metadata": {
        "id": "lNUGz-6gJwG0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "LiQxgK1Tojas",
        "outputId": "775820d9-ca7b-4e09-ea48-38965d20e47b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.13.3\n"
          ]
        }
      ],
      "source": [
        "!pip install tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import json\n",
        "import re\n",
        "from collections import Counter\n",
        "from operator import itemgetter\n",
        "from pprint import pprint\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import requests\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE, WordPiece\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import BpeTrainer, WordPieceTrainer\n"
      ],
      "metadata": {
        "id": "fds5q7EyovJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_corpus(file_name: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Reads a text file and returns a list of words.\n",
        "    \"\"\"\n",
        "    with open(file_name, \"r\", encoding=\"utf-8\") as file:\n",
        "        words_list = file.read().split()\n",
        "\n",
        "    return words_list"
      ],
      "metadata": {
        "id": "I59j7F4How_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_name: str = \"text.txt\"\n",
        "all_words: List[str] = read_corpus(file_name)\n",
        "print(all_words)"
      ],
      "metadata": {
        "id": "mDNTwMYPv_xk",
        "outputId": "0e89612b-969f-4730-ab77-2514aa447968",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['low', 'lower', 'newest', 'low', 'lower', 'newest', 'low', 'widest', 'newest', 'low', 'widest', 'newest', 'low', 'widest', 'newest']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions Used\n",
        "The code contains the following functions:\n",
        "- `read_corpus(file_name: str) -> List[str]`: Reads a text file and returns a list of words.\n",
        "- `word_frequency(words: List[str]) -> Dict[str, int]`: Calculates the frequency of words in a list.\n",
        "- `add_end_of_word_symbol(freqs: Dict[str, int]) -> Dict[str, int]`: Adds the end-of-word symbol \"\\</w>\" to each word in the frequency dictionary and returns a new dictionary with the modified keys.\n",
        "- `get_stats(word_frequencies: Dict[str, int]) -> Dict[Tuple[str, str], int]`: Returns a dictionary of symbol pairs and their frequencies in the given words.\n",
        "- `merge_vocab(pair: Tuple[str, str], vocab: Dict[str, int]) -> Dict[str, int]`: Merges a pair of symbols into a single symbol in a vocabulary dictionary.\n",
        "- `train(word_freq: Dict[str, int]) -> Dict[Tuple[str, str], int]`: Trains a byte pair encoding model on a word frequency dictionary.\n",
        "- `encode(original_word: str, all_merge_steps: Dict[Tuple[str, str], int]) -> Dict[str, int]`: Encodes a word using a byte pair encoding model."
      ],
      "metadata": {
        "id": "etom-hVRJ6Jl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def word_frequency(words: List[str]) -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    Calculate frequency of words in a list.\n",
        "    \"\"\"\n",
        "    count = dict(Counter(words))\n",
        "    return count"
      ],
      "metadata": {
        "id": "Z1dCVTVaow1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_freqs = word_frequency(all_words)\n",
        "\n",
        "print(word_freqs)"
      ],
      "metadata": {
        "id": "PtWZmWmFv7iA",
        "outputId": "a149d29c-0955-456b-deab-cc87b3c6e808",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'low': 5, 'lower': 2, 'newest': 5, 'widest': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_end_of_word_symbol(freqs: Dict[str, int]) -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    Adds the end-of-word symbol \"</w>\" to each word in the frequency dictionary\n",
        "    and returns a new dictionary with the modified keys.\n",
        "    \"\"\"\n",
        "    word_freq = {}\n",
        "    for word, freq in freqs.items():\n",
        "        word_freq[\" \".join(list(word)) + \" </w>\"] = freq\n",
        "    \n",
        "    return word_freq\n"
      ],
      "metadata": {
        "id": "ia_2gpTuowzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_freq = add_end_of_word_symbol(word_freqs)\n",
        "\n",
        "pprint(word_freq)"
      ],
      "metadata": {
        "id": "JhlouOtfv2hK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stats(word_frequencies: Dict[str, int]) -> Dict[Tuple[str, str], int]:\n",
        "    \"\"\"Returns a dictionary of symbol pairs and their frequencies in the given words.\n",
        "\n",
        "    Args:\n",
        "        word_frequencies: A dictionary where keys are words and values are their frequencies.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary where keys are symbol pairs and values are their frequencies.\n",
        "    \"\"\"\n",
        "    symbol_pairs_frequency = collections.defaultdict(int)\n",
        "\n",
        "    for word, frequency in word_frequencies.items():\n",
        "        symbols = word.split()\n",
        "\n",
        "        for i in range(len(symbols) - 1):\n",
        "            symbol_pair = (symbols[i], symbols[i + 1])\n",
        "            symbol_pairs_frequency[symbol_pair] += frequency\n",
        "\n",
        "    return symbol_pairs_frequency"
      ],
      "metadata": {
        "id": "DAci6dZSoww-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(get_stats(word_freq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TW84C13ZY90D",
        "outputId": "45c8f8f4-8d07-4883-9ebf-fbec601ae83f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<class 'int'>,\n",
            "            {('d', 'e'): 3,\n",
            "             ('e', 'r'): 2,\n",
            "             ('e', 's'): 8,\n",
            "             ('e', 'w'): 5,\n",
            "             ('i', 'd'): 3,\n",
            "             ('l', 'o'): 7,\n",
            "             ('n', 'e'): 5,\n",
            "             ('o', 'w'): 7,\n",
            "             ('r', '</w>'): 2,\n",
            "             ('s', 't'): 8,\n",
            "             ('t', '</w>'): 8,\n",
            "             ('w', '</w>'): 5,\n",
            "             ('w', 'e'): 7,\n",
            "             ('w', 'i'): 3})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_vocab(pair: Tuple[str, str], vocab: Dict[str, int]) -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    Merge a pair of symbols into a single symbol in a vocabulary dictionary.\n",
        "\n",
        "    Args:\n",
        "        pair: A tuple of two symbols to be merged.\n",
        "        vocab: A dictionary where the keys are words and the values are their frequencies.\n",
        "\n",
        "    Returns:\n",
        "        A new dictionary where each instance of the pair in the keys of the input dictionary has been replaced with the\n",
        "        concatenated string of the pair.\n",
        "    \"\"\"\n",
        "    pair_str = ' '.join(pair)\n",
        "    new_vocab = {\n",
        "        re.sub(re.escape(pair_str), ''.join(pair), word): freq for word, freq in vocab.items()\n",
        "        }    \n",
        "    \n",
        "    return new_vocab"
      ],
      "metadata": {
        "id": "9hlldjiQtFJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_MERGES = 10\n",
        "\n",
        "def train(word_freq: Dict[str, int]) -> Dict[Tuple[str, str], int]:\n",
        "    \"\"\"\n",
        "    Train a byte pair encoding model on a word frequency dictionary.\n",
        "\n",
        "    Args:\n",
        "        word_freq: A dictionary where the keys are words and the values are their frequencies.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary where the keys are symbol pairs (tuples) and the values are the iteration numbers at which they were merged.\n",
        "    \"\"\"\n",
        "    all_merge_steps: Dict[Tuple[str, str], int] = {}\n",
        "\n",
        "    for i, _ in enumerate(range(MAX_MERGES)):\n",
        "        pair_stats = get_stats(word_freq)\n",
        "        \n",
        "        if not pair_stats:\n",
        "            print(\"Reached maximum iterations\")\n",
        "            break\n",
        "\n",
        "        best_pair = max(pair_stats, key=lambda x: pair_stats[x])\n",
        "        all_merge_steps[best_pair] = i\n",
        "        print(f\"Most frequent pair at iteration {i + 1}: {best_pair}\")\n",
        "        word_freq = merge_vocab(best_pair, word_freq)\n",
        "        print(f\"Vocabulary at iteration {i + 1}:\")\n",
        "        print(json.dumps(word_freq, indent=4) + '\\n')\n",
        "\n",
        "    return all_merge_steps"
      ],
      "metadata": {
        "id": "YZgnAp17tFHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_merge_steps = train(word_freq)"
      ],
      "metadata": {
        "id": "Emc-S-1WvrE3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a59ce41b-0590-4bbf-fa4a-794fdcfeb85a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most frequent pair at iteration 1: ('e', 's')\n",
            "Vocabulary at iteration 1:\n",
            "{\n",
            "    \"l o w </w>\": 5,\n",
            "    \"l o w e r </w>\": 2,\n",
            "    \"n e w es t </w>\": 5,\n",
            "    \"w i d es t </w>\": 3\n",
            "}\n",
            "\n",
            "Most frequent pair at iteration 2: ('es', 't')\n",
            "Vocabulary at iteration 2:\n",
            "{\n",
            "    \"l o w </w>\": 5,\n",
            "    \"l o w e r </w>\": 2,\n",
            "    \"n e w est </w>\": 5,\n",
            "    \"w i d est </w>\": 3\n",
            "}\n",
            "\n",
            "Most frequent pair at iteration 3: ('est', '</w>')\n",
            "Vocabulary at iteration 3:\n",
            "{\n",
            "    \"l o w </w>\": 5,\n",
            "    \"l o w e r </w>\": 2,\n",
            "    \"n e w est</w>\": 5,\n",
            "    \"w i d est</w>\": 3\n",
            "}\n",
            "\n",
            "Most frequent pair at iteration 4: ('l', 'o')\n",
            "Vocabulary at iteration 4:\n",
            "{\n",
            "    \"lo w </w>\": 5,\n",
            "    \"lo w e r </w>\": 2,\n",
            "    \"n e w est</w>\": 5,\n",
            "    \"w i d est</w>\": 3\n",
            "}\n",
            "\n",
            "Most frequent pair at iteration 5: ('lo', 'w')\n",
            "Vocabulary at iteration 5:\n",
            "{\n",
            "    \"low </w>\": 5,\n",
            "    \"low e r </w>\": 2,\n",
            "    \"n e w est</w>\": 5,\n",
            "    \"w i d est</w>\": 3\n",
            "}\n",
            "\n",
            "Most frequent pair at iteration 6: ('low', '</w>')\n",
            "Vocabulary at iteration 6:\n",
            "{\n",
            "    \"low</w>\": 5,\n",
            "    \"low e r </w>\": 2,\n",
            "    \"n e w est</w>\": 5,\n",
            "    \"w i d est</w>\": 3\n",
            "}\n",
            "\n",
            "Most frequent pair at iteration 7: ('n', 'e')\n",
            "Vocabulary at iteration 7:\n",
            "{\n",
            "    \"low</w>\": 5,\n",
            "    \"low e r </w>\": 2,\n",
            "    \"ne w est</w>\": 5,\n",
            "    \"w i d est</w>\": 3\n",
            "}\n",
            "\n",
            "Most frequent pair at iteration 8: ('ne', 'w')\n",
            "Vocabulary at iteration 8:\n",
            "{\n",
            "    \"low</w>\": 5,\n",
            "    \"low e r </w>\": 2,\n",
            "    \"new est</w>\": 5,\n",
            "    \"w i d est</w>\": 3\n",
            "}\n",
            "\n",
            "Most frequent pair at iteration 9: ('new', 'est</w>')\n",
            "Vocabulary at iteration 9:\n",
            "{\n",
            "    \"low</w>\": 5,\n",
            "    \"low e r </w>\": 2,\n",
            "    \"newest</w>\": 5,\n",
            "    \"w i d est</w>\": 3\n",
            "}\n",
            "\n",
            "Most frequent pair at iteration 10: ('w', 'i')\n",
            "Vocabulary at iteration 10:\n",
            "{\n",
            "    \"low</w>\": 5,\n",
            "    \"low e r </w>\": 2,\n",
            "    \"newest</w>\": 5,\n",
            "    \"wi d est</w>\": 3\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(original_word: str, all_merge_steps: Dict[Tuple[str, str], int]) -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    Encode a word using a byte pair encoding model.\n",
        "\n",
        "    Args:\n",
        "        original_word: The word to be encoded.\n",
        "        all_merge_steps: A dictionary where the keys are symbol pairs (tuples) and the values are the iteration numbers at which they were merged.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary where the keys are the symbols in the encoded word and the values are their frequencies.\n",
        "    \"\"\"\n",
        "    if len(original_word) == 1:\n",
        "        return {original_word: 1}\n",
        "\n",
        "    vocab: Dict[str, int] = {original_word: 1}\n",
        "    vocab = add_end_of_word_symbol(vocab)\n",
        "    print(f'{vocab=}')\n",
        "\n",
        "    candidate_pairs = []\n",
        "    print(f'{all_merge_steps=}\\n')\n",
        "\n",
        "    while True:\n",
        "        symbol_pairs = get_stats(vocab)\n",
        "        print(f'{symbol_pairs=}')\n",
        "        candidate_pairs = [(pair, all_merge_steps[pair]) for pair in symbol_pairs if pair in all_merge_steps]\n",
        "        print(f'{candidate_pairs=}')\n",
        "\n",
        "        \n",
        "        if not candidate_pairs:\n",
        "            break\n",
        "        \n",
        "        best_pair = min(candidate_pairs, key=itemgetter(1))[0]\n",
        "        print(f\"pair to merge: {best_pair}\\n\")\n",
        "        vocab = merge_vocab(best_pair, vocab)\n",
        "\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "dhbqqtX8tFE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_word = 'lowest'\n",
        "encode(original_word, all_merge_steps)"
      ],
      "metadata": {
        "id": "KXgXsnrTtFC0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29eaa6d8-2da9-451d-84d0-58a70c16c649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab={'l o w e s t </w>': 1}\n",
            "all_merge_steps={('e', 's'): 0, ('es', 't'): 1, ('est', '</w>'): 2, ('l', 'o'): 3, ('lo', 'w'): 4, ('low', '</w>'): 5, ('n', 'e'): 6, ('ne', 'w'): 7, ('new', 'est</w>'): 8, ('w', 'i'): 9}\n",
            "\n",
            "symbol_pairs=defaultdict(<class 'int'>, {('l', 'o'): 1, ('o', 'w'): 1, ('w', 'e'): 1, ('e', 's'): 1, ('s', 't'): 1, ('t', '</w>'): 1})\n",
            "candidate_pairs=[(('l', 'o'), 3), (('e', 's'), 0)]\n",
            "pair to merge: ('e', 's')\n",
            "\n",
            "symbol_pairs=defaultdict(<class 'int'>, {('l', 'o'): 1, ('o', 'w'): 1, ('w', 'es'): 1, ('es', 't'): 1, ('t', '</w>'): 1})\n",
            "candidate_pairs=[(('l', 'o'), 3), (('es', 't'), 1)]\n",
            "pair to merge: ('es', 't')\n",
            "\n",
            "symbol_pairs=defaultdict(<class 'int'>, {('l', 'o'): 1, ('o', 'w'): 1, ('w', 'est'): 1, ('est', '</w>'): 1})\n",
            "candidate_pairs=[(('l', 'o'), 3), (('est', '</w>'), 2)]\n",
            "pair to merge: ('est', '</w>')\n",
            "\n",
            "symbol_pairs=defaultdict(<class 'int'>, {('l', 'o'): 1, ('o', 'w'): 1, ('w', 'est</w>'): 1})\n",
            "candidate_pairs=[(('l', 'o'), 3)]\n",
            "pair to merge: ('l', 'o')\n",
            "\n",
            "symbol_pairs=defaultdict(<class 'int'>, {('lo', 'w'): 1, ('w', 'est</w>'): 1})\n",
            "candidate_pairs=[(('lo', 'w'), 4)]\n",
            "pair to merge: ('lo', 'w')\n",
            "\n",
            "symbol_pairs=defaultdict(<class 'int'>, {('low', 'est</w>'): 1})\n",
            "candidate_pairs=[]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'low est</w>': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation of a BPE (Byte Pair Encoding) tokenizer using the HuggingFace Tokenizers library in Python.\n",
        "\n",
        "### BPE Tokenizer Implementation\n",
        "\n",
        "I implemented a Byte Pair Encoding (BPE) tokenizer using the Hugging Face tokenizers library. I initialized a BPE tokenizer and trainer, then trained the tokenizer on a sample text file. After training, I saved the trained tokenizer to a JSON file and printed the vocabulary size and top 20 tokens."
      ],
      "metadata": {
        "id": "Pd9gdZmmlTh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize BPE tokenizer and trainer\n",
        "bpe_text_box_tokenizer = Tokenizer(BPE())\n",
        "bpe_text_box_tokenizer.pre_tokenizer = Whitespace()\n",
        "bpe_text_box_trainer = BpeTrainer()\n",
        "\n",
        "# Train tokenizer on sample text files\n",
        "files = [\"./Sample.txt\"]\n",
        "bpe_text_box_tokenizer.train(files, bpe_text_box_trainer)\n",
        "\n",
        "bpe_text_box_tokenizer.save(\"./BPE-text-box.json\")\n",
        "\n",
        "vocab_size = bpe_text_box_tokenizer.get_vocab_size()\n",
        "tokens = bpe_text_box_tokenizer.get_vocab()\n",
        "tokens = sorted(tokens.items(), key=lambda x: x[1])\n",
        "\n",
        "print(\"BPE - TextBox Tokenizer Results\")\n",
        "print(\"--------------------------------\")\n",
        "print(f\"Vocabulary Size: {vocab_size}\")\n",
        "print(\"Tokens:\")\n",
        "pprint(tokens[:20])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvrF932LlK9Y",
        "outputId": "95037c7f-2e29-41b1-9941-ef94ffc438ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BPE - TextBox Tokenizer Results\n",
            "--------------------------------\n",
            "Vocabulary Size: 113\n",
            "Tokens:\n",
            "[('!', 0),\n",
            " ('.', 1),\n",
            " ('?', 2),\n",
            " ('E', 3),\n",
            " ('L', 4),\n",
            " ('N', 5),\n",
            " ('P', 6),\n",
            " ('T', 7),\n",
            " ('W', 8),\n",
            " ('a', 9),\n",
            " ('b', 10),\n",
            " ('c', 11),\n",
            " ('d', 12),\n",
            " ('e', 13),\n",
            " ('f', 14),\n",
            " ('g', 15),\n",
            " ('h', 16),\n",
            " ('i', 17),\n",
            " ('k', 18),\n",
            " ('l', 19)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WordPiece Tokenizer Implementation\n",
        "\n",
        "I implemented a WordPiece tokenizer using the same tokenizers library. I initialized a WordPiece tokenizer and trainer, then trained the tokenizer on a sample text file. After training, I saved the trained tokenizer to a JSON file and printed the vocabulary size and top 20 tokens."
      ],
      "metadata": {
        "id": "A7Dx0uIPosF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and configure the WordPiece tokenizer\n",
        "wp_text_box_tokenizer = Tokenizer(WordPiece())\n",
        "wp_text_box_tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "# Train the WordPiece tokenizer on the sample text file\n",
        "wp_text_box_trainer = WordPieceTrainer()\n",
        "files = [f\"./Sample.txt\"]\n",
        "wp_text_box_tokenizer.train(files, wp_text_box_trainer)\n",
        "\n",
        "# Save the trained tokenizer to a JSON file\n",
        "wp_text_box_tokenizer.save(\"./WordPiece-text-box.json\")\n",
        "\n",
        "vocab_size = wp_text_box_tokenizer.get_vocab_size()\n",
        "tokens = wp_text_box_tokenizer.get_vocab()\n",
        "tokens = sorted(tokens.items(), key=lambda x: x[1])\n",
        "\n",
        "print(\"WordPiece - TextBox Tokenizer Results\")\n",
        "print(\"--------------------------------\")\n",
        "print(f\"Vocabulary Size: {vocab_size}\")\n",
        "print(\"Tokens:\")\n",
        "pprint(tokens[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LMxB_dznmpM",
        "outputId": "b64de91a-c471-49e9-a9e5-81c95e08403f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WordPiece - TextBox Tokenizer Results\n",
            "--------------------------------\n",
            "Vocabulary Size: 139\n",
            "Tokens:\n",
            "[('!', 0),\n",
            " ('.', 1),\n",
            " ('?', 2),\n",
            " ('E', 3),\n",
            " ('L', 4),\n",
            " ('N', 5),\n",
            " ('P', 6),\n",
            " ('T', 7),\n",
            " ('W', 8),\n",
            " ('a', 9),\n",
            " ('b', 10),\n",
            " ('c', 11),\n",
            " ('d', 12),\n",
            " ('e', 13),\n",
            " ('f', 14),\n",
            " ('g', 15),\n",
            " ('h', 16),\n",
            " ('i', 17),\n",
            " ('k', 18),\n",
            " ('l', 19)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BPE and WordPiece Tokenization of Romeo and Juliet Book\n",
        "\n",
        "I tokenized sentences from the Romeo and Juliet book using both BPE and WordPiece tokenization. I initialized a BPE tokenizer and trainer, then trained the tokenizer on the Romeo and Juliet book. After training, I saved the trained tokenizer to a JSON file and printed the number of extracted tokens. Then, I initialized a WordPiece tokenizer and trainer, trained the tokenizer on the same book, saved the trained tokenizer to a JSON file, and printed the number of extracted tokens. Finally, I tokenized a sentence from the book using both the BPE and WordPiece tokenizers and printed the tokenization results."
      ],
      "metadata": {
        "id": "nq1Vz1R8taTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://www.gutenberg.org/cache/epub/1513/pg1513.txt\"\n",
        "rj_file = requests.get(url, allow_redirects=True).content\n",
        "\n",
        "open('./Romeo_and_Juliet.txt', 'wb').write(rj_file);"
      ],
      "metadata": {
        "id": "jW3QEkx7o0Re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BPE"
      ],
      "metadata": {
        "id": "Q1cQH3Xdu4Zo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize BPE tokenizer\n",
        "bpe_rj_tokenizer = Tokenizer(BPE())\n",
        "bpe_rj_tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "# Initialize trainer with special token [UNK]\n",
        "bpe_rj_trainer = BpeTrainer(special_tokens=[\"[UNK]\"])\n",
        "\n",
        "# Train tokenizer on input files\n",
        "rj_file = [\"./Romeo_and_Juliet.txt\"]\n",
        "bpe_rj_tokenizer.train(rj_file, bpe_rj_trainer)\n",
        "\n",
        "bpe_rj_tokenizer.save(\"./BPE-rj.json\")\n",
        "\n",
        "num_tokens = bpe_rj_tokenizer.get_vocab_size()\n",
        "print(f\"The number of extracted tokens (BPE - Romeo and Juliet Book): {num_tokens}\")"
      ],
      "metadata": {
        "id": "0zOD8bJVtt7Z",
        "outputId": "cc7ffc55-df77-4861-e17e-7349bd34aa3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of extracted tokens (BPE - Romeo and Juliet Book): 7216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WordPiece"
      ],
      "metadata": {
        "id": "4zzgbYSau6rf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize WordPiece tokenizer\n",
        "wp_rj_tokenizer = Tokenizer(WordPiece())\n",
        "wp_rj_tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "# Initialize trainer with special token [UNK]\n",
        "wp_rj_trainer = WordPieceTrainer(special_tokens=[\"[UNK]\"])\n",
        "\n",
        "# Train tokenizer on input files\n",
        "rj_file = [\"./Romeo_and_Juliet.txt\"]\n",
        "wp_rj_tokenizer.train(rj_file, wp_rj_trainer)\n",
        "\n",
        "# Save trained WordPiece tokenizer to file\n",
        "wp_rj_tokenizer.save(\"./WP-rj.json\")\n",
        "\n",
        "num_tokens = wp_rj_tokenizer.get_vocab_size()\n",
        "print(f\"The number of extracted tokens (WordPiece - Romeo and Juliet Book): {num_tokens}\")"
      ],
      "metadata": {
        "id": "8lDOZ9vGuJco",
        "outputId": "1179b653-c5cc-4815-ff01-bf1a069bf956",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of extracted tokens (WordPiece - Romeo and Juliet Book): 7759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply tokenizers on the sample text box"
      ],
      "metadata": {
        "id": "vSIQrnLCw58z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I loaded a short paragraph on tokenization from a text file called \"Sample.txt\" and stored it in a variable called \"text_box\". Then I applied Byte Pair Encoding (BPE) and WordPiece tokenization techniques on the text of the Romeo and Juliet book.\n",
        "\n",
        "To implement the tokenization models, I initialized BPE and WordPiece tokenizers and encoded the text of the book using each tokenizer. I stored the encoded outputs in variables called \"encoded_rj_text\" and \"encoded_wp_text\", respectively. After encoding the text, I printed the resulting encodings to the console along with the number of extracted tokens."
      ],
      "metadata": {
        "id": "dsNtSwZQOwuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_box = open(\"Sample.txt\", encoding=\"utf8\").read()\n",
        "text_box"
      ],
      "metadata": {
        "id": "GPJg81H1w8j2",
        "outputId": "994d2d02-f13c-43b3-f35a-f80dd13254fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This is a deep learning tokenization tutorial. Tokenization is the first step in a deep learning NLP pipeline. We will be comparing the tokens generated by each tokenization model. Excited much?! üòç'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BPE on Romeo and Juliet book"
      ],
      "metadata": {
        "id": "ORe6V9r8xICW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_rj_output = bpe_rj_tokenizer.encode(text_box)\n",
        "print(bpe_rj_output)\n",
        "print(f\"Tokens: {bpe_rj_output.tokens}\")"
      ],
      "metadata": {
        "id": "nPl7iZmJxN0J",
        "outputId": "7fb8447d-df6e-493c-d21d-e710da117329",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding(num_tokens=60, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
            "Tokens: ['This', 'is', 'a', 'deep', 'learning', 'to', 'ken', 'iz', 'ation', 'tutor', 'ial', '.', 'To', 'ken', 'iz', 'ation', 'is', 'the', 'first', 'step', 'in', 'a', 'deep', 'learning', 'N', 'L', 'P', 'pi', 'p', 'el', 'ine', '.', 'We', 'will', 'be', 'comp', 'aring', 'the', 'to', 'k', 'ens', 'gen', 'er', 'ated', 'by', 'each', 'to', 'ken', 'iz', 'ation', 'mo', 'de', 'l', '.', 'Ex', 'c', 'ited', 'much', '?', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WordPiece on Romeo and Juliet book"
      ],
      "metadata": {
        "id": "1tE_oE0_xj3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wp_rj_output = wp_rj_tokenizer.encode(text_box)\n",
        "print(wp_rj_output)\n",
        "print(f\"Tokens: {wp_rj_output.tokens}\")"
      ],
      "metadata": {
        "id": "3VcXo3DYxg2i",
        "outputId": "31746341-a469-4a34-cbb5-428ac7b9e8b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding(num_tokens=59, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
            "Tokens: ['This', 'is', 'a', 'deep', 'learning', 'to', '##ken', '##iz', '##ation', 'tutor', '##ial', '.', 'To', '##ken', '##iz', '##ation', 'is', 'the', 'first', 'step', 'in', 'a', 'deep', 'learning', 'N', '##L', '##P', 'pip', '##el', '##ine', '.', 'We', 'will', 'be', 'comp', '##aring', 'the', 'to', '##ken', '##s', 'gen', '##er', '##ated', 'by', 'each', 'to', '##ken', '##iz', '##ation', 'mod', '##el', '.', 'Ex', '##ci', '##te', '##d', 'much', '[UNK]', '[UNK]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply models on Romeo and Juliet book"
      ],
      "metadata": {
        "id": "aOQakILxx7SM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BPE"
      ],
      "metadata": {
        "id": "pu95Gwp6ylaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rj_text = open(\"./Romeo_and_Juliet.txt\").read()\n",
        "encoded_rj_text = bpe_rj_tokenizer.encode(rj_text)\n",
        "print(encoded_rj_text)"
      ],
      "metadata": {
        "id": "0tEGInRxx9S5",
        "outputId": "56a40397-4483-4d84-83bd-0297eb1d6a40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding(num_tokens=38285, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WordPiece"
      ],
      "metadata": {
        "id": "XzUkM5fwyv0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_wp_text = wp_rj_tokenizer.encode(rj_text)\n",
        "print(encoded_wp_text)"
      ],
      "metadata": {
        "id": "GoJBjwnVywTw",
        "outputId": "eedaae14-9bbd-4613-90dc-f7fe295dd593",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding(num_tokens=38285, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}